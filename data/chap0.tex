\chapter{引言}
\label{cha:intro}
    人工智能已经成为了人们生活中不可分割的一部分。近些年，各类机器学习模型，尤其是深度神经网络的模型越来越复杂，数据量越来越大。因此，深度神经网络的训练需要更多的计算资源以及更长的时间。
    
    无论使用云平台部署神经网络应用，或是使用本地集群资源，神经网络的训练过程通常都需要各类加速器，如GPU、TPU等。在Facebook于2017年的工作中，他们使用了256块GPU在一个小时内训练ResNet-50\cite{fb_imagenet}。现在，深度神经网络对计算资源的消耗将导致极高的使用成本。我们以阿里云的GPU实例为例，如表\ref{tab:ali_cloud}所示，在云上使用GPU资源的价格为每个GPU每小时十余元，在云上训练一个大型的神经网络需要的花费将达到几千或上万元。因此，对深度神经网络的运行时间进行预测，帮助控制训练成本，是一个非常重要的问题。
    
    \begin{table}[!htbp]
        \centering
	    \caption{阿里云GPU实例价格参考}
        \label{tab:ali_cloud}
        \begin{tabular}{|l|l|l|l|l|l|}
            \hline
            规格族 & vCPU & RAM & GPU & CPU & 价格/h \\
            \hline
            gnv6 & 8 & 32 GiB & NVIDIA V100 & Intel Xeon Platinum 8163 & 26.46元 \\
            \hline
            gn5 & 4 & 30 GiB & NVIDIA P100 & Intel Xeon E5-2682v4 & 12.78元 \\
            \hline
            gn5 & 8 & 60 GiB & NVIDIA P100 & Intel Xeon E5-2682v4 & 15.39元 \\
            \hline
            gn4 & 4 & 30 GiB & NVIDIA M40 & Intel Xeon E5-2682v4 & 10.88元 \\
            \hline
            gn4 & 8 & 30 GiB & NVIDIA M40 & Intel Xeon E5-2682v4 & 12.41元 \\
            \hline
            gn4 & 8 & 60 GiB & NVIDIA M40 * 2 & Intel Xeon E5-2682v4 & 21.76元 \\
            \hline
            gnv5i & 2 & 8 GiB & Nvidia Tesla P4 & Intel Xeon E5-2682v4 & 8.68元 \\
            \hline
            gnv5i & 4 & 16 GiB & Nvidia Tesla P4 & Intel Xeon E5-2682v4 & 9.69元 \\
            \hline
            gnv5i & 8 & 32 GiB & Nvidia Tesla P4 & Intel Xeon E5-2682v4 & 11.67元 \\
            \hline
        \end{tabular}
    \end{table}

    另一方面，如何选择更合适的配置进行训练，也是一个非常重要的问题。例如当我们在云上进行训练时，常常遇到这样的问题，在预算有限的情况下，如何选择训练配置，才能更快的训练，或是在时间有限的情况下，如何选择训练配置，才能更加节省开销。这需要对不同配置下深度神经网络的运行时间进行准确预测。另外，即使使用本地计算资源，在多个任务抢占计算资源时，也需要对资源进行合理分配，才能提高资源的利用效率。这时候，准确的性能预测模型，就成为了分配算法中非常重要的一部分。因此，对深度神经网络的运行时间进行预测，能够帮助合理化配置与资源分配，提高资源利用率。
    
    此外，更加准确的性能预测，能够协助超参数的调整，如批量大小、学习率等参数。从而提高计算资源的利用效率，提高模型整体性能。
    
    综上所述，预测深度神经网络的运行时间，是一个非常重要的问题。
    
    过去，人们对神经网络的性能预测做过很多尝试，一个传统的预测方式是，统计神经网络模型中的浮点运算个数（FLOPs），除以运行设备每秒能处理的浮点运算个数（FLOPS）得到模型的运行时间。而这种方法存在很多问题。首先，神经网络的模型越来越复杂，执行过程中操作间存在很多依赖关系，设备存在很多的空闲时间。此外，即使在操作的运行过程中，设备的计算资源也不一定被完全利用。因此，传统的预测方式，通常只能在运行时间的变化趋势上有一定的参考价值，并不能准确预测模型的运行时间。以TensorFlow自带的cost analyzer工具为例，该工具就是以上文提到的传统方式实现的，我们在\ref{sec:cmp}这一部分对其进行了测试，得到结论：传统模型的运行准确度不高，无法满足我们的使用。
    
    我们针对传统模型存在的问题，提出了我们的基于“预测-调度”框架的性能预测模型。首先，我们以操作为粒度，对{\bfseries 每个操作进行性能建模}，得到每个操作的预测运行时间。之后，我们将深度神经网络表示为数据流图的形式，{\bfseries 模拟数据流图的调度运行过程}，得到操作的执行顺序，以及并发情况，代入操作的预测运行时间，得到整体模型的运行时间。
    
    性能模型部分，即对各个操作进行性能建模。可以很好地解决操作在设备上使用资源不完全时地性能预测问题。我们对特定设备下，操作在不同参数下的运行时间进行测试，通过这些测试结果进行拟合，建立性能模型，从而预测不同参数下操作的运行时间，这样我们就不需要考虑运行过程中，设备的资源使用情况，只需要关心运行时间就可以了。另外，性能模型作为我们整个预测工具的主要模块，还有其他的功能。在准确的性能预测模型的帮助下，我们可以设计更好的调度算法，控制各个操作的调度顺序，更好地加速训练过程。
    
    调度模拟部分，在各个机器学习框架中，神经网络模型都被表示为数据流图的形式。数据流图可以清晰地表示操作之间的依赖关系，以及操作之间的并发情况。因此，我们对神经网络对应的数据流图进行调度模拟，就可以很好地还原机器学习框架中，神经网络应用的运行状况。从而准确地预测模型的运行时间。
    
    我们提出的深度神经网络性能预测模型，能够很好地预测神经网络任务的运行时间。与TensorFlow的预测工具cost analyzer相比，我们的预测准确度均高于cost analyzer，在最佳的测试中，我们的预测准确度达到cost analyzer的100倍以上。

    文章的主要贡献如下：

    \begin{itemize}
        \setlength{\itemindent}{1em}
        \item 提出了基于“预测-调度”模式的性能预测框架
        \item 基于提出的预测框架，构建了性能预测工具，得到较好的预测结果
    \end{itemize}
