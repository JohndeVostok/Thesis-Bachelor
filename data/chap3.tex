\chapter{实现}
\label{cha:impl}

\section{调度模拟}
    调度模拟部分主要处理四部分工作：
    \begin{enumerate}
        \setlength{\itemindent}{1em}
        \item {\bfseries 生成}：将用户输入的TensorFlow代码转化为数据流图的形式。
        \item {\bfseries 剪枝}：根据输入输出列表对数据流图进行剪枝，得到最小依赖集。
        \item {\bfseries 模拟}：动态模拟模型运行状况。
        \item {\bfseries 预测}：根据设定好的配置代入性能模型数据，预测性能。
    \end{enumerate}
    
    下面分别就四部分工作的实现进行讲解。

\subsection{生成}
    数据流图的生成，我们通过直接调用TensorFlow接口的方式进行。具体地，用户在按照正常使用TensorFlow的方式定义所需要的运算，创建会话（session），这时模拟器通过所创建的会话，拿到会话生成的默认图（default graph），再转化为图定义（graphdef）的形式，以JSON格式保存，作为下一步的输入。
    
    其中，通过直接转化得到的数据主要包括图的结构信息。例如图的所有节点列表，边列表，连接方式等。在我们的模型中，图以节点列表的形式进行组织。每个节点保存以下几项信息：节点名称，节点操作种类，节点入边集，节点出边集。
    
    由于直接通过TensorFlow中用户对计算过程的定义创建的图是不具有输入规模数据的，在使用模型的时候，用户还需要手动规定输入数据规模，作为后续操作提供参数。其中，输入规模参数与操作定义绑定。要求用户在定义计算操作的时候指定运算操作名称（name），在输入规模参数的同时通过运算操作名称定位数据流图中的节点，从而得到完整的数据流图及其参数。
    
\subsection{剪枝}
    剪枝操作在TensorFlow中是以深度优先搜索（DFS）进行的，在我的实验中，为了配合我的图存储方式，我使用广度优先搜索（BFS）进行实现。从计算输出点开始，反向进行广度优先搜索。没有被覆盖到的节点直接删除。
    
    实际使用的情况下，正常定义的卷积神经网络是不需要这一步操作的，因为正常定义的网络并没有冗余节点。

\subsection{模拟}
    在得到剪枝后的模型后，我们需要根据TensorFlow实际的运行方式进行模拟，以求在静态环境下尽量还原TensorFlow的动态调度过程。
    
    模拟过程中，我们首先会创建若干资源标记，包括CPU线程和GPU。以我们的测试环境为例，我们的测试环境包含32个CPU线程和4个GPU。由于TensorFlow的会话在进行的时候，会尽量占用所有的系统资源，因此这些资源标记可以用来确定某个操作运行的时候能够占用多少资源。
    
    正式开始模拟时，我们按照之前得到的操作运行顺序进行。每一轮迭代，先根据可用的资源，放置操作到运行区，即添加操作直到资源不够或不能按规定满足操作的要求为止。之后对操作进行模拟，根据操作占用的资源以及操作的参数调用预测模型的数据，得到操作的期望运行时间。这时候使用运行区操作中运行时间最快的操作推进整体时间，再进入下一轮迭代。这样我们就能比较准确地模拟TensorFlow中操作的动态调度过程。
    
    实际使用TensorFlow运行计算任务的时候，由于每个操作的运行时间并不稳定，调度过程可能并不能完全一致，因此是不能直接拿到调度方案的。另外于预测性能和实际性能依然会存在不一致的情况，这也会影响模拟调度的准确程度。但是，由于卷积神经网络的分层性，层间互不影响，层内也互不影响，因此实际使用中我们的调度模拟策略能够非常准确的还原实际运行状况，这一点我将在后文ref中进行分析。
    
\subsection{预测}
    作为整个预测过程中最为重要的部分，我们的操作性能模型是通过预测管理器进行的。预测管理器是一个键值对存储。每个操作的操作名称和运行的设备作为键（key），对应操作在相应硬件环境下的性能模型函数作为值（value）。因此在预测管理器的框架下，对操作性能模型的完善修改可以和调度模拟完全隔离开。
    
    在使用CPU的时候，不光需要指定使用CPU，还需要指定使用多少线程进行运算。而使用GPU的时候无需做这类指定。
    
    在基础配置下，TensorFlow中使用多块GPU的时候，无论进行数据并行还是模型并行，都需要手动指定使用设备，规定到操作或层使用哪块GPU进行。而一个操作只会在同一个GPU上进行，因此，我们的性能模型只需要针对操作在单块GPU上的运行状况即可。
    
    性能预测函数的具体预测方法将在下文进行介绍。

\section{性能模型}
\label{sec:impl_model}
    接下来我将介绍性能模型部分。主要的研究方式是通过不同参数的运行数据拿到操作的基本性能数据，然后对性能数据进行分析建模，得到性能模型。
    
    在CPU上，数据主要通过TensorFlow profiler来获得。TensorFlow profiler可以获得程序运行时数据流图上节点的运行时间，我们可以非常方便的获得节点的运行数据，从而完成建模。
    
    而在GPU上，由于Stream Executor对操作的再拆分，TensorFlow profiler不总能拿到节点的运行数据，因此我们需要从其他方式获得操作的运行性能数据。我们使用NVIDIA profiling tools来获得显卡上的函数调用情况，从而恢复出节点的运行状况。另外，在使用GPU时，CPU和GPU之间的数据传输也是运行时间的一个重要来源，这部分时间使用NVIDIA profiling tools可以准确获得，从而准确对所需操作进行建模。
    
    实验平台：

        CPU：2个8核Intel Xeon CPU(E5-2620 v4)

        RAM：256GB

        GPU：4块Nvidia GPU（Tesla V100），使用NVLink连接
        
        OS：Ubuntu 18.04.1 LTS

\subsection{矩阵乘法}
    上文\ref{overview:matmul}中提到，矩阵乘法包含三个参数$ M $、$ N $、$ P $。
    
    我们首先考虑CPU的情况。
    
    如果使用$ M \times N \times P $作为参数，得到的性能数据如图\ref{fig:matmul_cpu_nmp}所示。我们可以看到，尽管整体趋势上，运行时间和$ N \times M \times P $为正比例关系，但是相同$ N \times M \times P $取值下，时间差距较大。最差情况下，相同取值下，方差会最长时间比最短时间长1.38倍，对应\ref{fig:matmul_cpu_nmp_rsd}中相对标准偏差接近0.3的数据。因此，$ N \times M \times P $不是一个好的参数取值。
    
    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/matmul_cpu_nmp.png}
        \caption{CPU矩阵乘法的性能数据（参数为$N \times M \times P $）}
        \label{fig:matmul_cpu_nmp}
    \end{figure}
    
    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/matmul_cpu_nmp_rsd.png}
        \caption{CPU矩阵乘法的相对标准偏差数据（参数为$ N \times M \times P $）}
        \label{fig:matmul_cpu_nmp_rsd}
    \end{figure}
    
    我们考虑矩阵乘法的并行运行过程，矩阵乘法被拆分成了$ N \times M $个向量点积，每个向量长度为$ P $。因此，我们把$ N \times M $和$ P $作为参数，这时我们发现数据相对$ N \times M $和$ P $都保持了比较好的正比例关系，同时，在固定取值的情况下，相对标准偏差低于0.1，表现在实际数据中，最长的时间相比最短的时间不超过50\%，可以满足我们的预测需求。因此，在CPU上最终的矩阵乘法性能模型采用$ N \times M $和$ P $作为参数，对性能数据进行线性插值，作为预测性能模型。
    
    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/matmul_cpu_nm_p.png}
        \caption{CPU矩阵乘法的性能数据（参数为$N \times M $和$ P $）}
        \label{fig:matmul_cpu_nm_p}
    \end{figure}

    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/matmul_cpu_nm_p_rsd.png}
        \caption{CPU矩阵乘法的相对标准偏差数据（参数为$ N \times M $和$ P $）}
        \label{fig:matmul_cpu_nm_p_rsd}
    \end{figure}

    下面我们来对GPU数据进行建模。首先我们观察以$ N \times M \times P $为参数的数据，如图\ref{fig:matmul_gpu_nmp}所示。我们可以观察到，在$ N \times M \times P $取值较大的时候，性能变化较少，而且很符合正比例函数。但是在取值较小的时候关系不明显，而且变化较大。
    
    我们考虑GPU计算矩阵乘法的过程，$ M \times N $代表线程数量，而GPU中线程数量和CPU相比大很多，以我们的实验平台V100为例，V100单卡支持线程数为5120，因此，在$ N \times M \times P $规模不同的时候，性能模型是不同的。
    
    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/matmul_gpu_nmp.png}
        \caption{GPU矩阵乘法的性能数据（参数为$N \times M \times P $）}
        \label{fig:matmul_gpu_nmp}
    \end{figure}

    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/matmul_gpu_nmp_rsd.png}
        \caption{GPU矩阵乘法的相对标准偏差数据（参数为$ N \times M \times P $）}
        \label{fig:matmul_gpu_nmp_rsd}
    \end{figure}
    
    因为在卷积神经网络中，一般全连接层规模较大，因此我们先考虑大规模的情况。我们把$ N $、$ M $、$ P $规模限制到128以上，性能数据如图\ref{fig:matmul_gpu_nmp_big}所示，我们可以看出，无论是数据的线性程度还是数据的变化范围都有了很大的提升。图\ref{fig:matmul_gpu_nmp_rsd_big}指出，从相对标准偏差值来看，相较图\ref{fig:matmul_gpu_nmp_rsd}，GPU在参数取值较大的时候，进使用$ N \times M \times P $为参数，就可以达到比较好的预测效果了。因此，我们在规模较大的时候，直接使用$ M \times N \times P $为参数，进行线性插值。
    
    在规模较小的时候，由于取值数量少，规模小，我们的测试可以较为密集，此时使用$ N $、$ M $、$ P $三个参数进行线性插值，也能得到比较好的预测效果。

    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/matmul_gpu_nmp_big.png}
        \caption{GPU矩阵乘法的性能数据（参数为$N \times M \times P $，$ N $、$ M $、$ P $取值128以上）}
        \label{fig:matmul_gpu_nmp_big}
    \end{figure}

    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/matmul_gpu_nmp_rsd_big.png}
        \caption{GPU矩阵乘法的相对标准偏差数据（参数为$ N \times M \times P $，$ N $、$ M $、$ P $取值128以上）}
        \label{fig:matmul_gpu_nmp_rsd_big}
    \end{figure}

\subsection{二维卷积}
\label{ssec:impl_conv}
    二维卷积操作的运算包含6个参数，图片数量（$ B $）、图片尺寸（$ W \times H $）、卷积核尺寸（$ W_K \times H_K $）、输入通道数（$ C_{in} $）、输出通道数（$ C_{out} $）、步长（$ W_S \times H_S $）。因此我们针对以上六个参数进行性能测试，得到在CPU和GPU上的性能数据。

    首先考虑CPU的情况，我们使用$ B \times W \times H \times W_K \times W_H \times W_S^{-1} \times H_S^{-1} \times C_{in} \times C_{out} $作为参数，得到性能数据如图\ref{fig:conv_cpu}所示。我们看到，参数取值相同时，数据变化范围较大，不能够直接使用。而我们经过观察，可以看出，性能数据图中隐约显示出4条平行的直线。而数据中取值有4种的参数仅有$ C_{in} $和$ C_{out} $，因此，我们取参数$ B \times W \times H \times W_K \times W_H \times W_S^{-1} \times H_S^{-1} \times C_{in} $和$ B \times W \times H \times W_K \times W_H \times W_S^{-1} \times H_S^{-1} \times C_{out} $。我们发现，当我们仅选$ B \times W \times H \times W_K \times W_H \times W_S^{-1} \times H_S^{-1} \times C_{in}  $为参数时图像变得较为清晰，如图\ref{fig:conv_cpu_fix0}所示。但依然有大量点不符合正比例函数。
    
    在二维卷积操作中，如果卷积核仅为一个点，那么二维卷积运算会退化为矩阵和数字的乘法，此时不会调用二维卷积操作的函数，因此，我们去除这类点，得到性能数据如图\ref{fig:conv_cpu_fix1}所示，此时平均相对标准偏差为0.18可以满足我们的需求。

    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/conv_cpu.png}
        \caption{CPU二维卷积的性能数据（参数为用$ B \times W \times H \times W_K \times W_H \times W_S^{-1} \times H_S^{-1} \times C_{in} \times C_{out} $）}
        \label{fig:conv_cpu}
    \end{figure}

    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/conv_cpu_fix0.png}
        \caption{CPU二维卷积的性能数据（参数为$ B \times W \times H \times W_K \times W_H \times W_S^{-1} \times H_S^{-1} \times C_{in}  $）}
        \label{fig:conv_cpu_fix0}
    \end{figure}

    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/conv_cpu_fix1.png}
        \caption{CPU二维卷积的性能数据（参数为$ B \times W \times H \times W_K \times W_H \times W_S^{-1} \times H_S^{-1} \times C_{in}  $，去除卷积核大小为1的情况）}
        \label{fig:conv_cpu_fix1}
    \end{figure}

    在GPU计算的时候，由于Stream Executor会在映射过程中，将二维卷积操作映射成不同的CuDNN函数，如显式、隐式的卷积函数以及添不添加优化等多种情况，因此，在GPU上二维卷积的性能规律不显著，如图\ref{fig:conv_gpu}所示。我们无法降低参数数量，只能对数据直接进行线性插值。
    
    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/conv_gpu.png}
        \caption{GPU二维卷积的性能数据（参数为$ B \times I^2 \times K^2 \times C_{in} \times C_{out} $）}
        \label{fig:conv_gpu}
    \end{figure}
    
    二维卷积操作梯度求解方面，由于二维卷积的梯度求解操作也是二维卷积操作，因此我们直接套用二维卷积建立的性能模型即可。其中，正向的卷积操作为：
    
    $$
        X_1 = Conv2D(X_0, F_0, S_0)
    $$
    
    其中$ F $为卷积核，$ S $为步长。那么，针对输入的梯度为：
    
    $$
        d_0 = Conv2D(X_1^T, d_1, [1, 1])
    $$
    
    其中$ X_1 $需要根据原先的Stride进行补0操作。那么这步操作实际运算相当于输入为本层输出，卷积核为本层梯度的一个卷积操作。
    
    针对卷积核的梯度为：
    
    $$
        dF_0 = Conv2D(d_1, X_0, [1, 1])
    $$

    这一操作实际运算相当于输入为本层梯度，卷积核为本层输入的一个卷积操作。
    
    综上，二维卷积的梯度求解可以转化为二维卷积运算本身，因此不需要额外进行性能建模。
    
\subsection{局部响应归一化}
    局部相应归一化的输入包含一个四维张量，但是在计算中，前三维会被合并为向量，因此输入的前三维作为一个参数传入。因此影响局部相应归一化操作的参数有三个，输入的前三维规模（$ M $）、第四维规模（$ N $）、深度半径（$ R $）。
    
    在CPU上，我们以$ M \times N $为参数，这时局部响应归一化的性能数据如图\ref{fig:lrn_cpu}所示，此时的平均相对标准偏差为0.15，满足我们的需求，因此我们对这个结果直接进行插值，作为我们的局部响应归一化函数的性能模型。
    
    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/lrn_cpu.png}
        \caption{CPU局部响应归一化的性能数据（参数为$ M \times N $）}
        \label{fig:lrn_cpu}
    \end{figure}

    在GPU上，局部响应归一化函数的性能数据如图\ref{fig:lrn_gpu}所示，此时平均相对标准偏差为0.14，满足我们的需求。

    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/lrn_gpu.png}
        \caption{GPU局部响应归一化的性能数据（参数为$ M \times N $）}
        \label{fig:lrn_gpu}
    \end{figure}

\subsection{数据传输}
    在使用GPU的情况下，我们还需要考虑CPU和GPU之间的数据传输问题。由于数据传输仅仅发生在模型运行的开始和结束时候，且多轮训练不需要重复进行，因此，我们对数据传输模型进行简化。我们计算传输数据大小，然后除以CPU与GPU之间的双向互联带宽。我们的测试平台GPU为Tesla V100，双向互联带宽为32GB/s。

\subsection{数据修正}
    之前讨论的性能模型中，都存在不很符合我们预测模型的数据点，对这类数据点，我们采用提前存储的方式进行处理。针对之前测试中离模型相差过远的数据点，我们测试它周围一段距离的数据。测试数据落在这个范围内时，我们使用这一范围内的数据点进行插值，作为最终的预测结果，从而避免之前建立的模型精度不足的情况。
    
    另外，由于常用的卷积神经网络中使用的卷积操作规模都比较相似，因此，我们将常用的操作参数也作为特殊节点进行测试，这样在测试的时候，我们就可以更容易地得到准确的性能数据。
    