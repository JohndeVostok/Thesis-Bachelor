\chapter{算法}
\label{cap:algorithm}
\section{总览}
    我们的深度神经网络性能预测工具，总体基于“预测-调度”模型进行构建，主要预测TensorFlow上，深度神经网络应用的运行时间，系统架构如图\ref{fig:arch}所示。首先，我们对操作进行性能建模，从而预测不同参数下，操作的运行时间。这一部分我们称之为性能模型部分。之后，对神经网络模型对应的数据流图进行调度模拟，得到操作间的依赖关系，以及操作在执行过程中的执行顺序，例如哪个操作先进行，哪个后进行，哪些可以同时进行等。进而结合性能模型，还原神经网络模型的运行时间。这一部分我们称之为调度模拟。接下来我将对这两部分的设计及研究方法进行介绍。

    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/arch.jpg}
        \caption{整体架构}
        \label{fig:arch}
    \end{figure}

    
\section{性能模型}
    本节中，我们将简单介绍性能模型的建模对象，以及建模方法，并就待建模的各个操作进行简单介绍。

\subsection{对象选择}
    TensorFlow中支持的操作有200多种，我们对每个操作都建立性能模型无论是从时间上，还是资源上都是不现实的。因此我们需要对操作进行筛选。

    在卷积神经网络中，模型结构通常是分层的，其中主要包含三类：卷积层、池化层、全连接层。而这三层中，主要使用的操作包括二维卷积、矩阵乘法等。而在实际的训练过程中，为了避免过拟合的出现，通常层间还需要插入归一化函数、Dropout函数等。因此，我们将在以上操作中选择合适的操作进行建模。
    
    AlexNet\cite{alexnet}是一个典型的卷积神经网络。我们以AlexNet在CPU上执行的时间轴数据为例，如图\ref{fig:alexnet_timeline}所示。我们可以观察到，其中二维卷积（包括梯度求解）、矩阵乘法、归一化（包括梯度求解）三个操作贡献了几乎所有的运行时间。事实上，这三个操作在该样例中的运行时间中占用了总运行时间的95\%以上。同样的，在其他卷积神经网络，例如LeNet5、ResNet50等网络中，以上结论同样成立。因此，我们只要针对这三个操作进行建模，就可以比较好地对各类卷积神经网络进行预测了。

    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/alexnet_timeline.png}
        \caption{AlexNet在CPU执行的时间轴数据}
        \label{fig:alexnet_timeline}
    \end{figure}

\subsection{建模方法}
\label{ssec:model}
    我们的性能模型中，我们以执行设备、操作的输入规模和各个参数作为性能模型的输入参数，输出为操作在该输入参数下的运行时间。我们的主要思路是，首先测试不同输入参数下，操作的运行时间，然后对得到的数据进行分段插值，从而得到我们的性能预测模型。
    
    建模过程中的首要挑战就是测试参数取值的选择。受时间和资源的限制，我们不可能对每个操作的每个取值均进行测试。通常，操作的输入参数很多，如二维卷积操作中最多可能存在9个参数。同时，参数的取值范围很大，同样以二维卷积为例，通常输入的图像大小在$ 4 \times 4 $到$ 512 \times 512 $之间。这时，在可以承受的测试组数范围内，对于其中一个参数，可选的取值个数就比较小了。这对我们的模型精确度是非常不利的。
    
    我们考虑操作参数共有$ T $个，可以接受的测试组数为$ N $。那么，假设每个参数测试取值个数相同，那么每个参数的取值个数为$ N^{\frac{1}{T}} $。这时候，如果我们发现，参数$ A $和参数$ B $是共同以$ A \times B $的形式作用于测试时间的，那么我们将这两个参数合并，这时候还按原先的方式分配参数取值，那么每个参数的取值个数将变化为$ N^{\frac{1}{T-1}} $，此时每个参数的取值个数就会有所提高，从而提高整体模型的准确程度。因此，合并参数是提高模型准确度的一个重要方法。
    
    另一方面，在实际在建模过程中，存在模型不一致的情况，如数据规模较大和较小的时候，运算过程中使用了不同的核函数导致性能模型不一致，这种时候，我们将对测试进行分块，对不同的块分别建立不同的性能模型，以提高模型准确度。
    
    我们实际的研究过程是，先分别对每个操作进行取值测试，通过对测试结果的分析，进行参数合并或分段，再对合并后的部分操作进行进一步取值测试。对最终结果进行插值，作为操作性能模型。
    
    针对不同设备上，每个操作的建模参数选择过程，我们将在\ref{sec:impl_model}中详细介绍。

    此外，另一个重要的挑战就是准确的运行时间的获得。测试TensorFlow中操作的运行时间的方法主要有三种：
    
    \begin{itemize}
        \item {\bfseries 会话计时} 在会话中执行该参数下的单操作模型，并对会话运行时间进行计时。这种方法的主要问题是，会计入大量计算时间以外的时间，如GPU中的数据传输时间或大测试数据的生成时间。
        \item {\bfseries TensorFlow Profiler} TensorFlow        Profiler是TensorFlow提供的分析工具，可以帮助采集神经网络在运行过程中的各类性能信息，如数据流图上各个点的运行时间。我们主要采用的也是这个功能。这个方法的问题在于，部分操作的部分核函数在执行过程中不会被TensorFlow Profiler追踪到，因此无法获得运行准确时间。
        \item {\bfseries NVIDIA Profiling tools} NVIDIA Profiling tools是NVIDIA提供的分析工具，简称nvprof。nvprof可以分析进程在运行过程中的CUDA函数调用。这一方法的好处在于时间准确，没有额外时间，精确到每个函数调用。缺点在于，只能对GPU进行测试，另外部分操作在执行过程中会调用多个CUDA函数，而且个数不定，内容不定，无法直接分析结果。
    \end{itemize}
    
    因此，实际使用中，我们需要因地制宜，在特定设备上使用特定的计时方式。
    
    针对不同设备上，每个操作的计时方式选择，我们将在\ref{sec:impl_model}中详细介绍。
    
    接下来我将分别介绍三种不同的操作的定义及执行细节。
    
\subsection{矩阵乘法}
\label{ssec:view_matmul}
    全连接层是卷积神经网络中非常重要的组成部分，通常出现在卷积神经网络的最后几层，将卷积层得到的局部特征进行综合。全连接层的操作就是二维矩阵的乘法。相比其他操作，全连接层的参数量大，运行时间长。因此，对矩阵乘法进行性能建模是非常有必要的。
    
    矩阵乘法的输入为两个矩阵，即两个二维张量。其中矩阵$ A $的形状为$ M \times P $，矩阵$ B $的形状为$ P \times N $，输出为矩阵$ C = A \times B $，形状为$M \times N$。其中，$ C_{m, n} = \sum_{p=1}^P{a_{m, p} \times b_{p, n}} $。
    
    在单线程运行情况下，矩阵乘法的复杂度为$ O(M N P) $。然而，由于数据的局部性，即使$ M \times N \times P $固定，将两个参数交换，如交换$ N $和$ P $，矩阵乘法操作的运行时间是不同的。我们在建模的时候不能仅仅选取$ M \times N \times P $作为参数。
    
    在CPU多线程计算或使用GPU计算的情况下，一般会对矩阵乘法运算进行展开。这种情况下任务会被转化为$ M \times N $个$ P $维向量点积，因此在较大规模的情况下，我们可以合并$ M $和$ N $，用来提高预测精度。这一部分将会在\ref{ssec:impl_matmul}节进行详细讨论。

    在TensorFlow中，系统会根据运算的规模和使用的硬件平台，将矩阵乘法映射到不同的函数。以CPU为例，数据规模较小的时候，TensorFlow会直接进行计算，而数据规模较大的时候系统可能会调用Eigen\cite{eigen}中的函数。在GPU中，系统会根据规模的不同选用不同的CuBlas函数进行计算，我们的性能预测模型需要针对这一性质有所考虑。

    此外，矩阵乘法的梯度求解依然是矩阵乘法。在TensorFlow中，矩阵乘法的梯度求解并没有作为一个单独的操作出现，因此我们不需要考虑矩阵乘法的梯度函数建模问题。


\subsection{二维卷积}
\label{ssec:view_conv}
    二维卷积是卷积神经网络的核心运算，占用了卷积神经网络中最多的运算时间。通常来说，卷积层和池化层是成对出现的。卷积层通过二维卷积运算提取局部特征，池化层通过选取局部最大这类操作减少参数规模。由于池化操作并不包含运算，因此在TensorFlow中，二维卷积和池化操作是合并进行的。因此，我们只需要测试二维卷积的运行时间，就可以很好地预测卷积层和池化层的运行时间了。

    在TensorFlow中使用二维卷积操作，需要三个张量作为输入：输入数据、卷积核、步长。其中，输入数据是一个四维张量。我们常用一组图片来描述这个张量，第一维代表图片的数量，第二、第三维代表图片的宽、高，第四维代表图片的通道数量，如灰度图片通道数为1，RGB图片通道数为3。而在中间层中，这一输入通常用来描述一组特征图。这时候第一维依然对应原图中的图片编号，第二维第三维标记了特征对应的位置，而第四维是特征向量维度。这时我们可以把这一输入看作，特征向量的集合。卷积核也是一个四维张量，第一、第二维表示卷积核的宽、高，第三维表示输入通道数，第四维表示输出通道数。步长也是一个四维张量，表示卷积核的移动步长，通常情况下，我们需要对每张图片都进行处理，同时我们需要特征向量的所有数据。因此，步长这一输入的第一维和第四维均固定取值为1。步长的第二维和第三维通常用来减小输出的规模，减少重复的特征信息，降低参数量。此外，一般情况下，二维卷积操作还需要指定填充方式，由于这个参数对性能影响不大，我们在建模中默认采用相同填充的方式。

    二维卷积的计算可以看作若干组二维的点积。
    
    在每张图片，每个通道中，我们以$ X_1 = conv(X_0, K, [W_s, H_s]) $为例，其中$ X $为一个$ W \times H $的图片，$ K $为$ W_k \times H_k $的卷积核，那么，有：
    $$
        X_1[x, y] = \sum_{i=0}^{i < W_k}\sum_{j=0}^{j < H_k}X[W_s \times x + i, H_s \times y + 1] \times K[i, j]
    $$
    
    这步操作的时间复杂度为：
    $$
        O\left(\frac{W \times H \times W_k \times H_k}{W_s \times H_s}\right)
    $$

    接下来我们考虑卷积操作的梯度函数，我们在训练过程中需要两个梯度：卷积核的梯度和上一层的梯度。因此，二维卷积操作需要两个梯度函数。在TensorFlow中这两个函数分别为Conv2DBackPropFilter和Conv2DBackPropInput。
    
    我们已知输出梯度为$ \delta_1 $，其中$ \delta_1 $已经根据本层的步长进行补0，这时卷积核的梯度计算如下：
    $$
        \delta_K = conv(X_0, \delta_1)
    $$
    输入的梯度计算如下：
    $$
        \delta_0 = conv(\delta_1, K^T)
    $$
    
    两种梯度函数均为二维卷积操作，我们在系统中只对卷积操作进行性能建模，两种梯度函数转化为卷积操作直接进行预测。
    
    二维卷积运算还可以转化为矩阵乘法\cite{im2col}。我们将卷积中窗口每次的计算映射到矩阵中的一次行与列的点积。输入中的每次二维点积对应的元素作为向量成为矩阵中的行，总共执行的窗口数作为矩阵的列。同样，卷积核也进行类似操作。这样卷积操作就可以转化为矩阵乘法。这种方式转化可以提高运算的并行性，更好的用GPU进行加速。
    
    在TensorFlow中，系统会根据操作所在的设备和数据规模决定使用的二维卷积算法，尤其在GPU上，同时存在多种映射方式，这给我们的性能建模带来很大的挑战。


\subsection{局部响应归一化}
    局部响应归一化AlexNet中提出的一种操作，用来避免模型的过拟合。尽管在后人的工作\cite{vggnet}中发现中发现，局部响应归一化并不能达到我们预期的效果，但是在我们的工作中，为了更好地模拟卷积神经网络的运行过程，我们依然对这个操作进行性能建模。
    
    局部响应归一化顾名思义是一个归一化函数，需要一个输入和4个参数。其中输入是一个四维张量，局部响应归一化函数是针对其中的第四维进行归一化。4个参数分别是深度半径（$ r $）、偏移量（$ b $）、$ \alpha $、$ \beta $。
    我们记一次归一化如下：
    $$
        Y = S(X)
    $$
    那么，有：
    $$
        Z[i, j, k, w] = sum(X[i, j, k, w-r: w+r+1]^2)
    $$
    $$
        Y = \frac{Z}{(b + \alpha * Z)^\beta}
    $$

    因此，局部响应归一化的运行时间仅与输入规模和深度半径取值相关。

\section{调度模拟}
    本节中，我将简单介绍调度模拟部分的结构，以及TensorFlow中数据流图的执行流程。

    调度模拟的输入分为两部分，待预测的深度神经网络模型和运行的硬件平台参数。
    
    其中，输入中的待预测模型不仅需要提供计算的定义，还需要提供输入数据的规模参数。定义网络的时候，通常不需要在模型定义的时候严格限定输入的规模。批量大小（Batch-size）通常也是训练过程中需要调整的一个重要超参数。如图\ref{fig:dag_same}所示，在这种情况下，输入规模的变化并不会影响数据流图的生成。因此，用户在使用性能预测工具的时候，还需要额外输入数据规模的相关参数。

    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/dag_same.jpg}
        \caption{模型相同，输入规模不同，数据流图相同}
        \label{fig:dag_same}
    \end{figure}

    此外，用户还需要输入运行的硬件平台参数，如CPU、GPU的型号与数量，方便调度模拟部分根据资源进行调度。
    
    该模块在调用性能模型后输出整个神经网络的预测时间。

    在上文中，我们提到，TensorFlow中的模型会被转化为数据流图的形式。数据流图是一个有向无环图，图上的点表示一个操作，图上的一条边表示一个张量数据或一条依赖。我们的调度模拟模块目标就是将用户输入的深度神经网络模型转化为数据流图，再按照TensorFlow的运行过程模拟它的调度过程。

    TensorFlow中，运行一个模型需要通过会话进行。用户首先需要创建一个会话，与此同时，之前定义的模型被转化为了数据流图的形式。接下来，用户执行会话的运行功能，这时TensorFlow开始执行数据流图上的操作。数据流图在从创建到运行的过程中，经历了六个阶段：图的创建、图的传送、图的剪枝、图的划分、图的二次划分、图的运行。下面我将分别对各个部分进行解释。

    {\bfseries 图的创建} 根据用户输入的模型创建数据流图。用户在使用TensorFlow的时候，以运算的形式定义神经网络，如生成数据、或对数据套用某些操作。这种情况下，用户的输入可以直接对应数据流图中的一个子图。以图\ref{fig:dag_mat}中定义的网络为例，用户通过random\_normal操作生成两个$ 5000 \times 5000 $ 的矩阵$ a $和$ b $。之后定义$ c = a \times b $，$ d = b * c $，最终输出为矩阵$ d $。系统会先按照用户的输入生成一个数据流图，如图\ref{fig:dag_mat}左下显示。之后，将部分节点映射成为一个子图，这样就得到了用户定义模型的数据流图。

    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/dag_mat.jpg}
        \caption{数据流图的创建过程}
        \label{fig:dag_mat}
    \end{figure}

    {\bfseries 图的传送} 将数据流图序列化并发送。用户创建的会话并不直接执行数据流图的计算，而是将图先发送给主机（master），主机再将任务划分发送给从机（worker）去执行。这一步，先将数据流图转化为一种易于传输的形式，再进行发送。
    
    {\bfseries 图的剪枝} 求图的最小依赖集。这一步在主机上进行，首先将得到的序列转化为数据流图。根据之前给定的输入输出列表，对图进行剪枝，删除不需要被执行的节点。在TensorFlow的实现中，这一步使用深度优先搜索进行。

    {\bfseries 图的划分} 将图分配到不同从机。 在TensorFlow中进行多机并行的时候，需要手动对计算指定运行的从机。这部分的工作就是根据用户的指定，将图切分，并传输到相应的从机上。

    {\bfseries 图的二次划分} 将图分配到不同设备。这一步在从机上进行。TensorFlow会根据当前可用的设备，和用户对操作指定的设备，将图再次切分，将各部分分配到不同设备上，默认情况下，TensorFlow会把操作尽量分给GPU。在单机多GPU的情况下，用户需要手动指定操作执行的GPU编号。
    
    {\bfseries 图的运行} 运行子图。从机为每个子图创建一个执行器对象，用来运行这个子图。经过两次划分，现有的子图可以直接在同一个设备上执行。执行器等待依赖的数据，数据准备完毕后直接执行。执行器就是Stream Executor的实例，对操作进行封装。
    
    调度模拟部分主要工作就是执行以上流程，从图的创建开始，到两次划分为止，图的运行被替换成了调用性能模型。我们的实验主要在单机上进行，因此图的传送和第一次划分在我们的系统中没有相应的实现。
